from dotenv import load_dotenv
import streamlit as st
from langchain import HuggingFaceHub
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_core.callbacks import StdOutCallbackHandler
import os

def main():
    load_dotenv()
    st.set_page_config(page_title="ğŸ’¬ íŒŒì´ì¬ ì½”ë“œ ë„ìš°ë¯¸ ì±—ë´‡")
    st.header("ğŸ‘¨â€ğŸ’» íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì§ˆë¬¸ ì±—ë´‡ (ë¬´ë£Œ Hugging Face ê¸°ë°˜)")

    if "conversation" not in st.session_state:
        st.session_state.conversation = get_conversation_chain()
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    user_question = st.chat_input("íŒŒì´ì¬ ì½”ë“œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
    if user_question:
        handle_user_input(user_question)

def get_conversation_chain():
    llm = HuggingFaceHub(
        repo_id="mistralai/Mistral-7B-Instruct-v0.1",  # Chat êµ¬ì¡° ì§€ì›
        model_kwargs={"temperature": 0.7, "max_new_tokens": 512}
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    conversation_chain = ConversationChain(
        llm=llm,
        memory=memory,
        verbose=True,
        callbacks=[StdOutCallbackHandler()]
    )
    return conversation_chain

def handle_user_input(question):
    response = st.session_state.conversation.run(question)
    st.session_state.chat_history = st.session_state.conversation.memory.chat_memory.messages

    for i, msg in enumerate(st.session_state.chat_history):
        role = "user" if msg.type == "human" else "assistant"
        with st.chat_message(role):
            st.markdown(msg.content)

if __name__ == "__main__":
    main()
