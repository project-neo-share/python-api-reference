import streamlit as st
from huggingface_hub import InferenceClient
import os

# í† í° ë¶ˆëŸ¬ì˜¤ê¸° (í•„ìˆ˜)
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Hugging Face Inference API í´ë¼ì´ì–¸íŠ¸
client = InferenceClient(
    model="tiiuae/falcon-rw-1b",
    token=os.getenv("HUGGINGFACEHUB_API_TOKEN")
)
st.set_page_config(page_title="ğŸ íŒŒì´ì¬ ì½”ë“œ ì§ˆë¬¸ ì±—ë´‡", layout="wide")
st.title("ğŸ’¬ íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ë„ìš°ë¯¸ (Hugging Face ë¬´ë£Œ API)")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ì‚¬ìš©ì ì…ë ¥
user_prompt = st.chat_input("íŒŒì´ì¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë³´ì„¸ìš”!")

# ëŒ€í™” ì¶œë ¥
for role, content in st.session_state.chat_history:
    with st.chat_message(role):
        st.markdown(content)

# ì‘ë‹µ ì²˜ë¦¬
if user_prompt:
    st.session_state.chat_history.append(("user", user_prompt))
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ëª…ì‹œì  role prompt ì‚½ì…)
    full_prompt = (
        "You are a helpful assistant who explains and writes Python code.\n\n"
        f"User: {user_prompt}\nAssistant:"
    )

    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  Mistral ëª¨ë¸ì´ ë‹µë³€ ì¤‘ì…ë‹ˆë‹¤..."):
            response = client.text_generation(
                prompt=full_prompt,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.95
            )
            st.markdown(response.strip())
            st.session_state.chat_history.append(("assistant", response.strip()))
